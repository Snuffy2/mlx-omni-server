[project]
name = "mlx-omni-server"
version = "0.5.1"
description = "MLX Omni Server is a server that provides OpenAI-compatible APIs using Apple's MLX framework."
authors = [{ name = "madroid", email = "madroidmaq@gmail.com" }]
requires-python = ">=3.11,<3.13" # outlines (v1.2.8) does not support Python 3.13
readme = "README.md"
license = "MIT"
keywords = [
    "mlx",
    "ai",
    "agi",
    "aigc",
    "server",
    "openai",
    "tts",
    "stt",
]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "Operating System :: OS Independent",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
]
dependencies = [
    # core
    "fastapi>=0.116.1,<1",
    "pydantic>=2.9.2,<3",
    "uvicorn>=0.34.0,<1",
    "rich>=13.9.4",
    # chat
    "mlx-lm>=0.28.2",
    "outlines>=1.0.4,<2",
    # models
    "huggingface-hub>=0.30,<2",
    # audio
    "f5-tts-mlx>=0.2.5,<0.3",
    "mlx-whisper>=0.4.1",
    "mlx-audio>=0.2.4",
    # images
    "mflux>=0.11.0,<0.12",
    # embeddings
    "mlx-embeddings>=0.0.3",
]

[project.urls]
Repository = "https://github.com/madroidmaq/mlx-omni-server"

[project.scripts]
mlx-omni-server = "mlx_omni_server.main:start"

[dependency-groups]
dev = [
    "pytest>=8.3.3,<10",
    "pre-commit>=4.0.1",
    "black>=25",
    "isort>=6",
    "weave>=0.51.46,<0.60",
    "openai>=1.97.0",
    "anthropic>=0.57.1",
]

[tool.hatch.build.targets.sdist]
include = ["src/mlx_omni_server"]

[tool.hatch.build.targets.wheel]
include = ["src/mlx_omni_server"]

[tool.hatch.build.targets.wheel.sources]
"src/mlx_omni_server" = "mlx_omni_server"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.pytest.ini_options]
log_cli = true
log_cli_level = "INFO"
log_cli_format = "%(asctime)s [%(levelname)8s] %(message)s (%(filename)s:%(lineno)s)"
log_cli_date_format = "%Y-%m-%d %H:%M:%S"

[tool.black]
line-length = 88

[tool.isort]
profile = "black"
line_length = 88
